{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sanskrit_tutor_title"
   },
   "source": [
    "# Sanskrit Tutor - QLoRA Fine-tuning\n",
    "\n",
    "This notebook allows you to fine-tune a language model on your Sanskrit text corpus using QLoRA (Quantized Low-Rank Adaptation).\n",
    "\n",
    "## ‚ö†Ô∏è IMPORTANT: User Assets Required\n",
    "\n",
    "This notebook expects you to have uploaded your Sanskrit corpus and model files to Google Drive. The required structure is:\n",
    "\n",
    "```\n",
    "drive/MyDrive/sanskrit-tutor-user-assets/\n",
    "‚îú‚îÄ‚îÄ passages.jsonl\n",
    "‚îú‚îÄ‚îÄ qa_pairs.jsonl  \n",
    "‚îú‚îÄ‚îÄ config.yaml\n",
    "‚îî‚îÄ‚îÄ models/ (optional - for base models)\n",
    "```\n",
    "\n",
    "**This notebook will NOT download data or models for you.** You must provide them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_requirements"
   },
   "outputs": [],
   "source": [
    "# Install required packages for fine-tuning\n",
    "!pip install -q \\\n",
    "    transformers>=4.30.0 \\\n",
    "    peft>=0.4.0 \\\n",
    "    bitsandbytes>=0.39.0 \\\n",
    "    accelerate>=0.20.0 \\\n",
    "    datasets>=2.12.0 \\\n",
    "    torch>=2.0.0 \\\n",
    "    pyyaml \\\n",
    "    jsonlines\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define user assets path\n",
    "USER_ASSETS_PATH = Path('/content/drive/MyDrive/sanskrit-tutor-user-assets')\n",
    "print(f\"User assets expected at: {USER_ASSETS_PATH}\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if USER_ASSETS_PATH.exists():\n",
    "    print(\"‚úÖ User assets directory found!\")\n",
    "    print(\"Contents:\")\n",
    "    for item in USER_ASSETS_PATH.iterdir():\n",
    "        print(f\"  {item.name}\")\n",
    "else:\n",
    "    print(\"‚ùå User assets directory not found!\")\n",
    "    print(\"Please create the directory and upload your files according to the README.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validation_section"
   },
   "source": [
    "## 2. Validate User Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_assets"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "def validate_user_assets(assets_path: Path):\n",
    "    \"\"\"\n",
    "    Validate that required user assets are present and properly formatted.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    # Check required files\n",
    "    required_files = {\n",
    "        'passages.jsonl': 'Sanskrit passages for fine-tuning',\n",
    "        'qa_pairs.jsonl': 'Question-answer pairs for training',\n",
    "        'config.yaml': 'Configuration file'\n",
    "    }\n",
    "    \n",
    "    for filename, description in required_files.items():\n",
    "        filepath = assets_path / filename\n",
    "        if not filepath.exists():\n",
    "            errors.append(f\"Missing {filename}: {description}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Found {filename}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(\"\\n‚ùå Validation failed:\")\n",
    "        for error in errors:\n",
    "            print(f\"  - {error}\")\n",
    "        return False\n",
    "    \n",
    "    # Validate file contents\n",
    "    try:\n",
    "        # Check passages.jsonl\n",
    "        passages_file = assets_path / 'passages.jsonl'\n",
    "        with open(passages_file, 'r', encoding='utf-8') as f:\n",
    "            passage_count = 0\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                if line.strip():\n",
    "                    passage_count += 1\n",
    "                    if line_num <= 3:  # Check first 3 lines for required fields\n",
    "                        try:\n",
    "                            obj = json.loads(line)\n",
    "                            required_fields = ['id', 'text_devanagari', 'text_iast', 'work']\n",
    "                            for field in required_fields:\n",
    "                                if field not in obj:\n",
    "                                    errors.append(f\"passages.jsonl line {line_num}: missing field '{field}'\")\n",
    "                        except json.JSONDecodeError:\n",
    "                            errors.append(f\"passages.jsonl line {line_num}: invalid JSON\")\n",
    "        \n",
    "        print(f\"üìö Found {passage_count} passages\")\n",
    "        \n",
    "        # Check qa_pairs.jsonl\n",
    "        qa_file = assets_path / 'qa_pairs.jsonl'\n",
    "        with open(qa_file, 'r', encoding='utf-8') as f:\n",
    "            qa_count = 0\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                if line.strip():\n",
    "                    qa_count += 1\n",
    "                    if line_num <= 3:  # Check first 3 lines\n",
    "                        try:\n",
    "                            obj = json.loads(line)\n",
    "                            required_fields = ['id', 'question', 'answer']\n",
    "                            for field in required_fields:\n",
    "                                if field not in obj:\n",
    "                                    errors.append(f\"qa_pairs.jsonl line {line_num}: missing field '{field}'\")\n",
    "                        except json.JSONDecodeError:\n",
    "                            errors.append(f\"qa_pairs.jsonl line {line_num}: invalid JSON\")\n",
    "        \n",
    "        print(f\"‚ùì Found {qa_count} QA pairs\")\n",
    "        \n",
    "        # Check config.yaml\n",
    "        config_file = assets_path / 'config.yaml'\n",
    "        with open(config_file, 'r', encoding='utf-8') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            print(f\"‚öôÔ∏è Configuration loaded\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        errors.append(f\"Error reading files: {str(e)}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(\"\\n‚ùå Validation errors:\")\n",
    "        for error in errors:\n",
    "            print(f\"  - {error}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"\\n‚úÖ All user assets validated successfully!\")\n",
    "    return True\n",
    "\n",
    "# Run validation\n",
    "validation_success = validate_user_assets(USER_ASSETS_PATH)\n",
    "\n",
    "if not validation_success:\n",
    "    print(\"\\nüõë Please fix the validation errors before proceeding.\")\n",
    "    print(\"\\nFor help with the required file formats, see:\")\n",
    "    print(\"https://github.com/yourusername/sanskrit-tutor/blob/main/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_prep_section"
   },
   "source": [
    "## 3. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_data"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import re\n",
    "\n",
    "def load_training_data(assets_path: Path):\n",
    "    \"\"\"\n",
    "    Load and prepare training data from user assets.\n",
    "    \"\"\"\n",
    "    # Load passages\n",
    "    passages = {}\n",
    "    with open(assets_path / 'passages.jsonl', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                passage = json.loads(line)\n",
    "                passages[passage['id']] = passage\n",
    "    \n",
    "    print(f\"Loaded {len(passages)} passages\")\n",
    "    \n",
    "    # Load QA pairs and create training examples\n",
    "    training_examples = []\n",
    "    \n",
    "    with open(assets_path / 'qa_pairs.jsonl', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                qa = json.loads(line)\n",
    "                \n",
    "                # Create context from related passages\n",
    "                context_parts = []\n",
    "                if 'related_passage_ids' in qa:\n",
    "                    for passage_id in qa['related_passage_ids']:\n",
    "                        if passage_id in passages:\n",
    "                            p = passages[passage_id]\n",
    "                            context_part = f\"[{p['id']}] {p['work']} {p['chapter']}.{p['verse']}\\n\"\n",
    "                            context_part += f\"Devanagari: {p['text_devanagari']}\\n\"\n",
    "                            context_part += f\"IAST: {p['text_iast']}\"\n",
    "                            if p.get('notes'):\n",
    "                                context_part += f\"\\nNotes: {p['notes']}\"\n",
    "                            context_parts.append(context_part)\n",
    "                \n",
    "                context = \"\\n\\n\".join(context_parts) if context_parts else \"No specific passages provided.\"\n",
    "                \n",
    "                # Format as instruction-following example\n",
    "                instruction = f\"\"\"You are a Sanskrit tutor. Answer the following question using the provided context and cite your sources using the exact passage IDs in square brackets.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {qa['question']}\"\"\"\n",
    "                \n",
    "                response = qa['answer']\n",
    "                \n",
    "                training_examples.append({\n",
    "                    'instruction': instruction,\n",
    "                    'response': response,\n",
    "                    'qa_id': qa['id'],\n",
    "                    'difficulty': qa.get('difficulty', 'unknown')\n",
    "                })\n",
    "    \n",
    "    print(f\"Created {len(training_examples)} training examples\")\n",
    "    \n",
    "    # Convert to dataset\n",
    "    dataset = Dataset.from_list(training_examples)\n",
    "    \n",
    "    # Print some statistics\n",
    "    difficulties = [ex['difficulty'] for ex in training_examples]\n",
    "    difficulty_counts = pd.Series(difficulties).value_counts()\n",
    "    print(f\"\\nDifficulty distribution:\")\n",
    "    print(difficulty_counts)\n",
    "    \n",
    "    return dataset, passages\n",
    "\n",
    "# Only proceed if validation passed\n",
    "if validation_success:\n",
    "    training_dataset, passage_dict = load_training_data(USER_ASSETS_PATH)\n",
    "    print(f\"\\nüìä Training dataset ready: {len(training_dataset)} examples\")\n",
    "    \n",
    "    # Show a sample\n",
    "    print(\"\\nüìù Sample training example:\")\n",
    "    sample = training_dataset[0]\n",
    "    print(f\"Instruction: {sample['instruction'][:200]}...\")\n",
    "    print(f\"Response: {sample['response'][:200]}...\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without valid user assets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_setup_section"
   },
   "source": [
    "## 4. Setup Base Model and QLoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_model"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"microsoft/DialoGPT-small\"  # Default small model for testing\n",
    "# For better results, consider: \"mistralai/Mistral-7B-Instruct-v0.1\" or \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# Note: Larger models require more GPU memory\n",
    "\n",
    "def setup_model_and_tokenizer(model_name: str):\n",
    "    \"\"\"\n",
    "    Setup the base model with 4-bit quantization and LoRA configuration.\n",
    "    \"\"\"\n",
    "    print(f\"Setting up model: {model_name}\")\n",
    "    \n",
    "    # Quantization configuration for QLoRA\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    # Load model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=16,  # Rank of adaptation\n",
    "        lora_alpha=32,  # LoRA scaling parameter\n",
    "        lora_dropout=0.1,  # LoRA dropout\n",
    "        target_modules=[\"c_attn\", \"c_proj\"],  # Target modules for DialoGPT\n",
    "        # For Mistral/Llama, use: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print(f\"‚úÖ Model and tokenizer loaded with QLoRA configuration\")\n",
    "    print(f\"üìä Trainable parameters: {model.print_trainable_parameters()}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "if validation_success:\n",
    "    # Check GPU availability\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if device.type == \"cuda\":\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = setup_model_and_tokenizer(MODEL_NAME)\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without valid user assets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_section"
   },
   "source": [
    "## 5. Training Configuration and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_setup"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "def format_training_example(example, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Format a training example for instruction following.\n",
    "    \"\"\"\n",
    "    # Create the full training text\n",
    "    full_text = f\"{example['instruction']}\\n\\n{example['response']}{tokenizer.eos_token}\"\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # For causal language modeling, labels are the same as input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Tokenize the entire dataset.\n",
    "    \"\"\"\n",
    "    def tokenize_function(examples):\n",
    "        # Process each example\n",
    "        results = {\n",
    "            \"input_ids\": [],\n",
    "            \"attention_mask\": [],\n",
    "            \"labels\": []\n",
    "        }\n",
    "        \n",
    "        for i in range(len(examples['instruction'])):\n",
    "            example = {\n",
    "                'instruction': examples['instruction'][i],\n",
    "                'response': examples['response'][i]\n",
    "            }\n",
    "            \n",
    "            tokenized = format_training_example(example, tokenizer, max_length)\n",
    "            \n",
    "            results[\"input_ids\"].append(tokenized[\"input_ids\"][0])\n",
    "            results[\"attention_mask\"].append(tokenized[\"attention_mask\"][0])\n",
    "            results[\"labels\"].append(tokenized[\"labels\"][0])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Tokenizing dataset\"\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "if validation_success and 'model' in locals():\n",
    "    # Tokenize the dataset\n",
    "    tokenized_dataset = tokenize_dataset(training_dataset, tokenizer, max_length=512)\n",
    "    \n",
    "    # Split into train/validation\n",
    "    train_test_split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset = train_test_split['train']\n",
    "    eval_dataset = train_test_split['test']\n",
    "    \n",
    "    print(f\"üìä Training set: {len(train_dataset)} examples\")\n",
    "    print(f\"üìä Validation set: {len(eval_dataset)} examples\")\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # Not masked language modeling\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Training data prepared\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without valid model setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_model_section"
   },
   "source": [
    "## 6. Fine-tuning with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "def setup_training_arguments(output_dir=\"./sanskrit-tutor-qlora\"):\n",
    "    \"\"\"\n",
    "    Setup training arguments for QLoRA fine-tuning.\n",
    "    \"\"\"\n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=2,  # Small batch size for Colab\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 2 * 4 = 8\n",
    "        num_train_epochs=3,  # Start with few epochs\n",
    "        learning_rate=2e-4,  # Higher learning rate for LoRA\n",
    "        warmup_steps=100,\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=None,  # Disable wandb/tensorboard\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=False,\n",
    "        fp16=True,  # Use mixed precision for speed\n",
    "        ddp_find_unused_parameters=False,\n",
    "    )\n",
    "\n",
    "if validation_success and 'train_dataset' in locals():\n",
    "    # Setup training arguments\n",
    "    training_args = setup_training_arguments()\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Trainer initialized\")\n",
    "    print(f\"üìä Total training steps: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
    "    \n",
    "    # Start training\n",
    "    print(\"\\nüöÄ Starting fine-tuning...\")\n",
    "    print(\"This may take 30-60 minutes depending on your data size and GPU.\")\n",
    "    \n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"‚úÖ Training completed successfully!\")\n",
    "        \n",
    "        # Save the final model\n",
    "        trainer.save_model()\n",
    "        print(f\"üíæ Model saved to: {training_args.output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {str(e)}\")\n",
    "        print(\"This might be due to memory constraints. Try reducing batch size or sequence length.\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without properly prepared training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_model_section"
   },
   "source": [
    "## 7. Save Model to Drive and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model_to_drive(model, tokenizer, source_dir=\"./sanskrit-tutor-qlora\"):\n",
    "    \"\"\"\n",
    "    Save the fine-tuned model to Google Drive.\n",
    "    \"\"\"\n",
    "    # Create timestamped directory in Drive\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    drive_model_dir = USER_ASSETS_PATH / f\"fine_tuned_model_{timestamp}\"\n",
    "    drive_model_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"üíæ Saving model to: {drive_model_dir}\")\n",
    "    \n",
    "    # Save model and tokenizer\n",
    "    model.save_pretrained(drive_model_dir)\n",
    "    tokenizer.save_pretrained(drive_model_dir)\n",
    "    \n",
    "    # Copy training logs if they exist\n",
    "    if os.path.exists(source_dir):\n",
    "        for item in os.listdir(source_dir):\n",
    "            if item.endswith('.json') or item.endswith('.txt'):\n",
    "                shutil.copy2(os.path.join(source_dir, item), drive_model_dir / item)\n",
    "    \n",
    "    # Create a README for the saved model\n",
    "    readme_content = f\"\"\"# Sanskrit Tutor Fine-tuned Model\n",
    "\n",
    "**Training Date:** {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "**Base Model:** {MODEL_NAME}\n",
    "**Training Examples:** {len(training_dataset) if 'training_dataset' in locals() else 'Unknown'}\n",
    "**Method:** QLoRA (4-bit quantization + LoRA)\n",
    "\n",
    "## Usage\n",
    "\n",
    "This model can be loaded using the `transformers` and `peft` libraries:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model and tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"{MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{MODEL_NAME}\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"path/to/this/directory\")\n",
    "```\n",
    "\n",
    "## Files\n",
    "\n",
    "- `adapter_config.json`: LoRA adapter configuration\n",
    "- `adapter_model.safetensors`: LoRA adapter weights\n",
    "- `tokenizer.json`, `tokenizer_config.json`: Tokenizer files\n",
    "- Training logs and other metadata\n",
    "\"\"\"\n",
    "    \n",
    "    with open(drive_model_dir / \"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    print(f\"‚úÖ Model saved successfully to Drive!\")\n",
    "    print(f\"üìÅ Location: {drive_model_dir}\")\n",
    "    \n",
    "    return drive_model_dir\n",
    "\n",
    "# Test the fine-tuned model\n",
    "def test_model(model, tokenizer, test_question=\"What does 'dharma' mean?\"):\n",
    "    \"\"\"\n",
    "    Test the fine-tuned model with a sample question.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüß™ Testing model with question: '{test_question}'\")\n",
    "    \n",
    "    # Create a test prompt in the same format as training\n",
    "    test_prompt = f\"\"\"You are a Sanskrit tutor. Answer the following question and cite your sources using exact passage IDs in square brackets.\n",
    "\n",
    "Question: {test_question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True, max_length=400)\n",
    "    \n",
    "    # Move inputs to the same device as model\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the generated part\n",
    "    answer_start = response.find(\"Answer:\") + len(\"Answer:\")\n",
    "    generated_answer = response[answer_start:].strip()\n",
    "    \n",
    "    print(\"\\nüìù Generated Response:\")\n",
    "    print(\"=\"*50)\n",
    "    print(generated_answer)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return generated_answer\n",
    "\n",
    "# Save and test if training was successful\n",
    "if 'trainer' in locals() and hasattr(trainer, 'model'):\n",
    "    # Save to Drive\n",
    "    saved_model_path = save_model_to_drive(trainer.model, tokenizer)\n",
    "    \n",
    "    # Test the model\n",
    "    test_model(trainer.model, tokenizer)\n",
    "    \n",
    "    print(f\"\\nüéâ Fine-tuning completed successfully!\")\n",
    "    print(f\"üìÅ Model saved to: {saved_model_path}\")\n",
    "    print(f\"\\nüîÑ To use this model in the Sanskrit Tutor:\")\n",
    "    print(f\"1. Copy the adapter files to your local user_assets/adapters/ directory\")\n",
    "    print(f\"2. Update your config.yaml to point to the adapter\")\n",
    "    print(f\"3. The system will automatically detect and load the adapter\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No trained model available to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion_section"
   },
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "### Using Your Fine-tuned Model\n",
    "\n",
    "1. **Download the adapter files** from your Google Drive to your local machine\n",
    "2. **Place them** in the `user_assets/adapters/` directory of your Sanskrit Tutor installation\n",
    "3. **Update your configuration** to use the fine-tuned model\n",
    "4. **Test the improved model** in the Sanskrit Tutor interface\n",
    "\n",
    "### Improving Results\n",
    "\n",
    "- **More data**: Add more passages and QA pairs to your training data\n",
    "- **Better base model**: Try larger models like Mistral-7B or Llama-2-7B\n",
    "- **Hyperparameter tuning**: Adjust learning rate, LoRA rank, and training epochs\n",
    "- **Data quality**: Ensure your QA pairs have accurate citations and high-quality answers\n",
    "\n",
    "### Monitoring and Evaluation\n",
    "\n",
    "- Check the training logs for loss curves\n",
    "- Test the model on held-out questions\n",
    "- Compare citation accuracy before and after fine-tuning\n",
    "- Monitor for overfitting (high train accuracy, low validation accuracy)\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: This notebook requires you to provide your own data and models. The fine-tuned model will only be as good as the training data you provide. For best results, ensure your passages.jsonl and qa_pairs.jsonl files are high-quality and representative of the questions you want the system to answer."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
