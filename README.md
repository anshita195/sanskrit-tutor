# 🕉️ Sanskrit Tutor - RAG-powered Learning System

A Retrieval-Augmented Generation (RAG) chatbot system for Sanskrit language learning, built with user-supplied data and configurable for local or cloud inference.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

## 🌟 Features

- **💬 Interactive Chat**: Ask questions about Sanskrit texts, grammar, and philosophy
- **📝 Exercise Mode**: Practice with guided questions and detailed feedback
- **🔍 Passage Lookup**: Explore specific text passages with citations
- **🎵 Audio Practice**: Upload recordings for pronunciation analysis (experimental)
- **📚 Citation-backed Answers**: All responses include exact passage references
- **🏠 Local or Cloud**: Run with local GGUF models or hosted APIs
- **🔧 Fine-tuning**: Optional LoRA fine-tuning with Google Colab

## ⚠️ IMPORTANT: How the user supplies data & models (MANDATORY)

**This system requires you to provide your own Sanskrit texts and models.** The code will NOT download datasets or models automatically. You must supply all required files according to the specifications below.

### Required File Structure

Create the following directory structure with your data:

```
sanskrit-tutor/
├── user_assets/                    # ← YOU MUST CREATE AND POPULATE
│   ├── passages.jsonl              # ← REQUIRED: Your Sanskrit texts
│   ├── qa_pairs.jsonl              # ← REQUIRED: Question-answer pairs
│   ├── config.yaml                 # ← REQUIRED: Configuration
│   ├── models/                     # ← OPTIONAL: Local GGUF models
│   │   └── your-model.gguf
│   ├── adapters/                   # ← OPTIONAL: LoRA adapters
│   └── audio_samples/              # ← OPTIONAL: Audio files
│       └── sample_001.wav
├── data/                           # ← Generated by the system
└── src/                            # ← Provided by this repository
```

### 1. Passages File (REQUIRED)

**File**: `user_assets/passages.jsonl`  
**Format**: UTF-8 JSON Lines (one JSON object per line)

```jsonl
{"id":"gretil_shloka_001","text_devanagari":"त्वमेव माता च पिता त्वमेव","text_iast":"tvameva mata ca pita tvameva","work":"GRETIL:Bhagavadgita","chapter":"2","verse":"20","language":"sanskrit","source_url":"https://example.org/gretil/bg/2/20","notes":"You alone are mother and father"}
{"id":"gretil_shloka_002","text_devanagari":"त्वमेव बन्धुश्च सखा त्वमेव","text_iast":"tvameva bandhusca sakha tvameva","work":"GRETIL:Bhagavadgita","chapter":"2","verse":"21","language":"sanskrit","source_url":"https://example.org/gretil/bg/2/21","notes":"You alone are relative and friend"}
```

**Required fields for each passage:**
- `id`: Unique identifier (used for citations)
- `text_devanagari`: Sanskrit text in Devanagari script
- `text_iast`: Sanskrit text in IAST transliteration
- `work`: Source work identifier
- `chapter`: Chapter/section number
- `verse`: Verse/line number
- `language`: Should be "sanskrit"
- `source_url`: URL to original source
- `notes`: Editorial notes or translation

**Minimum**: 50 passages recommended, 5+ for testing.

### 2. QA Pairs File (REQUIRED)

**File**: `user_assets/qa_pairs.jsonl`  
**Format**: UTF-8 JSON Lines

```jsonl
{"id":"qa_001","question":"What is the meaning of 'tvameva'?","answer":"You alone — tvam + eva. This emphasizes the singular devotion to the divine. [gretil_shloka_001]","difficulty":"easy","related_passage_ids":["gretil_shloka_001"]}
{"id":"qa_002","question":"What does 'mata ca pita' mean?","answer":"Mother and father. In this context, it expresses that the divine serves all familial roles. [gretil_shloka_001]","difficulty":"medium","related_passage_ids":["gretil_shloka_001"]}
```

**Required fields for each QA pair:**
- `id`: Unique identifier
- `question`: Learning question
- `answer`: Answer with citations in [passage_id] format
- `difficulty`: "easy", "medium", "hard", or custom levels
- `related_passage_ids`: Array of relevant passage IDs

**Minimum**: 50+ QA pairs recommended, 20+ for testing.

### 3. Configuration File (REQUIRED)

**File**: `user_assets/config.yaml`

```yaml
# Model Configuration
model_path: "user_assets/models/mistral-7b-instruct.gguf"  # or null
gguf_local: true  # false to use hosted APIs
n_ctx: 4096
n_gpu_layers: 0

# Embedding Configuration  
embeddings_model: "sentence-transformers/all-mpnet-base-v2"

# Data Paths
passages_file: "user_assets/passages.jsonl"
qa_file: "user_assets/qa_pairs.jsonl"
faiss_index_path: "data/faiss.index"

# Optional Features
audio_folder: "user_assets/audio_samples"

# RAG Parameters
retrieval_k: 5
max_tokens: 500
temperature: 0.7

# API Fallbacks (optional)
hf_model: "mistralai/Mistral-7B-Instruct-v0.1"
openai_model: "gpt-3.5-turbo"
```

### 4. Models (OPTIONAL - for local inference)

If you want to run models locally, place GGUF files in `user_assets/models/`:

**Recommended models to download:**
- **Mistral-7B-Instruct GGUF**: Download from [Hugging Face](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF)
  - Save as: `user_assets/models/mistral-7b-instruct.gguf`
- **Code Llama GGUF**: For better structured responses
  - Save as: `user_assets/models/codellama-7b-instruct.gguf`

**We do NOT download these for you.** Visit the links, download the `.gguf` files, and place them in the correct directory.

### 5. API Keys (for hosted inference fallback)

If you don't have local models, set environment variables:

```bash
export HF_API_KEY="hf_your_huggingface_api_key_here"
# OR
export OPENAI_API_KEY="sk_your_openai_api_key_here"
```

**We do NOT provide API keys.** You must obtain them from:
- Hugging Face: https://huggingface.co/settings/tokens
- OpenAI: https://platform.openai.com/api-keys

## 🚀 Installation

### Quick Start

```bash
# Clone the repository
git clone https://github.com/yourusername/sanskrit-tutor.git
cd sanskrit-tutor

# Run installation script
chmod +x install.sh
./install.sh

# Or install manually:
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### Additional Features

```bash
# For local GGUF models
pip install llama-cpp-python

# For GPU acceleration  
pip install faiss-gpu torch

# For audio features
pip install librosa soundfile torchaudio

# For fine-tuning
pip install transformers peft bitsandbytes accelerate datasets
```

## 📚 Usage

### 1. Validate Your Data

```bash
python src/utils/config_validator.py
```

This will check all your user assets and provide specific error messages if anything is missing or incorrectly formatted.

### 2. Build the Index

```bash
python src/embed_index.py --config user_assets/config.yaml
```

This creates embeddings and a FAISS search index from your passages.

### 3. Launch the UI

```bash
python src/ui_gradio.py --config user_assets/config.yaml
```

Opens a web interface at http://localhost:7860 with:
- **Chat Mode**: Ask questions and get cited answers
- **Exercise Mode**: Practice with your QA pairs
- **Passage Lookup**: Search by passage ID
- **Audio Practice**: Upload pronunciation samples

### 4. Command Line Usage

```bash
# Interactive RAG chat
python src/rag.py --config user_assets/config.yaml --interactive

# Test specific question
python src/rag.py --config user_assets/config.yaml --question "What is dharma?"

# Test LLM backends
python src/llm_backends.py --config user_assets/config.yaml
```

## 🧪 Testing

```bash
# Run all tests (uses test fixtures)
python -m pytest tests/ -v

# Test with your user assets (if provided)
python -m pytest tests/test_retrieval.py::test_user_assets_mode -v

# Skip tests requiring heavy dependencies
SKIP_EMBEDDING_TESTS=1 SKIP_INTEGRATION_TESTS=1 python -m pytest tests/ -v
```

## 🔧 Fine-tuning (Optional)

Use the provided Google Colab notebook to fine-tune models on your data:

1. Upload your `user_assets/` folder to Google Drive at `MyDrive/sanskrit-tutor-user-assets/`
2. Open `notebooks/colab_qlora.ipynb` in Google Colab
3. Follow the notebook instructions
4. Download the fine-tuned adapters to `user_assets/adapters/`

The system will automatically detect and load adapters when present.

## 🛠️ Configuration Options

### Model Settings

- **Local GGUF**: Set `gguf_local: true` and provide `model_path`
- **Hosted API**: Set `gguf_local: false` and provide API keys
- **Hybrid**: System automatically falls back to hosted APIs if local model fails

### RAG Settings

- `retrieval_k`: Number of passages to retrieve (default: 5)
- `max_tokens`: Maximum response length (default: 500)
- `temperature`: Response randomness (default: 0.7)

### Performance Settings

- `n_ctx`: Context window for local models (default: 4096)
- `n_gpu_layers`: GPU layers for local models (default: 0 = CPU only)

## 📁 Project Structure

```
sanskrit-tutor/
├── user_assets/           # Your data (you provide)
│   ├── passages.jsonl
│   ├── qa_pairs.jsonl
│   ├── config.yaml
│   └── models/
├── src/                   # Source code
│   ├── utils/
│   │   └── config_validator.py
│   ├── ingest.py
│   ├── embed_index.py
│   ├── llm_backends.py
│   ├── rag.py
│   └── ui_gradio.py
├── tests/                 # Test suite
│   ├── fixtures/          # Test data
│   └── test_retrieval.py
├── notebooks/             # Jupyter notebooks
│   └── colab_qlora.ipynb
├── data/                  # Generated files
├── requirements.txt
├── setup.py
└── README.md
```

## 🚨 Troubleshooting

### "No LLM backends available"

1. For local inference: Place a GGUF model in `user_assets/models/` and set `gguf_local: true`
2. For hosted inference: Set `gguf_local: false` and export API keys
3. Install dependencies: `pip install llama-cpp-python` (local) or `pip install requests` (hosted)

### "Missing user assets"

1. Create `user_assets/passages.jsonl` with your Sanskrit texts
2. Create `user_assets/qa_pairs.jsonl` with your questions
3. Create `user_assets/config.yaml` with your settings
4. Run `python src/utils/config_validator.py` for detailed guidance

### "Embedding model not available"

```bash
pip install sentence-transformers
```

### "FAISS index not found"

```bash
python src/embed_index.py --config user_assets/config.yaml
```

### Memory issues with local models

1. Reduce `n_ctx` in config.yaml
2. Use smaller GGUF models
3. Set `n_gpu_layers: 0` for CPU-only inference
4. Switch to hosted API fallback

## 🤝 Contributing

1. Fork the repository
2. Create your feature branch: `git checkout -b feature/amazing-feature`
3. Run tests: `python -m pytest tests/ -v`
4. Commit your changes: `git commit -m 'Add amazing feature'`
5. Push to the branch: `git push origin feature/amazing-feature`
6. Open a Pull Request

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 🙏 Acknowledgments

- Built with [Gradio](https://gradio.app/) for the web interface
- Uses [sentence-transformers](https://www.sbert.net/) for embeddings
- [FAISS](https://faiss.ai/) for efficient similarity search
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) for local inference
- Supports fine-tuning with [PEFT](https://github.com/huggingface/peft) and QLoRA

## ❓ FAQ

**Q: Where do I get Sanskrit texts?**  
A: You must provide your own texts in the required JSONL format. Consider digitizing texts from GRETIL, DCS, or other Sanskrit digital libraries (respecting copyright).

**Q: Can I use this for other languages?**  
A: Yes, but you'll need to modify the prompt templates and provide appropriate texts in your target language.

**Q: Do you provide pre-trained models?**  
A: No, you must provide your own GGUF models or API keys. We recommend Mistral-7B-Instruct or similar instruction-tuned models.

**Q: Is this suitable for production use?**  
A: This is a research/educational tool. For production, you'll need to add proper error handling, authentication, and scaling infrastructure.

**Q: Can I commercialize this?**  
A: The code is MIT licensed, but ensure you have rights to your training data and comply with any model licensing terms.

---

**🔗 Quick Links:**
- [Installation Script](install.sh)
- [Configuration Validator](src/utils/config_validator.py)
- [Test Suite](tests/test_retrieval.py)
- [Fine-tuning Notebook](notebooks/colab_qlora.ipynb)
- [Example Config](user_assets/config.yaml)

For more help, please open an issue on GitHub or consult the inline documentation in the source code.
