# Core dependencies - always required
pyyaml>=6.0
numpy>=1.21.0
pandas>=1.3.0
pathlib

# Data processing and validation
jsonlines>=3.0.0

# Embeddings and search
sentence-transformers>=2.2.0
faiss-cpu>=1.7.0  # Use faiss-gpu for GPU acceleration
scikit-learn>=1.0.0

# Web interface
gradio>=4.0.0
fastapi>=0.68.0
uvicorn>=0.15.0

# HTTP requests for hosted APIs
requests>=2.25.0
urllib3>=1.26.0

# Optional: Local GGUF inference (CPU-only version)
# Uncomment the line below if you want to use local GGUF models
# llama-cpp-python>=0.2.20

# Optional: GPU-accelerated GGUF inference
# For CUDA support, install manually with:
# pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121

# Optional: Audio processing (if using audio features)
librosa>=0.9.0
soundfile>=0.10.0
torch>=1.13.0  # Required by some audio models
torchaudio>=0.13.0

# Development and testing
pytest>=6.0.0
pytest-cov>=3.0.0
black>=22.0.0
flake8>=4.0.0

# Jupyter notebook support (for Colab)
jupyter>=1.0.0
ipywidgets>=7.6.0

# Optional: Fine-tuning dependencies (for LoRA/QLoRA)
# Uncomment if you plan to use the fine-tuning notebook
# transformers>=4.30.0
# peft>=0.4.0
# bitsandbytes>=0.39.0
# accelerate>=0.20.0
# datasets>=2.12.0

# Visualization and analysis
matplotlib>=3.5.0
seaborn>=0.11.0

# Text processing utilities
regex>=2022.0.0
unidecode>=1.3.0
